{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNrP8btMQ07-",
        "outputId": "8c7bd263-19d0-4144-ae99-9dd1676e726c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (6.0.4)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect) (1.10.7)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect) (4.5.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "!pip3 install inflect\n",
        "import json\n",
        "import nltk\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import torch\n",
        "import gensim\n",
        "import string\n",
        "import inflect\n",
        "import torch.nn.functional as F\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import display\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import words\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Embedding\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "p = inflect.engine()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_words = set(words.words())\n",
        "def remove_non_english_words(text):\n",
        "    words_list = text.split()\n",
        "    filtered_words = [word for word in words_list if word.lower() in english_words]\n",
        "    return ' '.join(filtered_words)"
      ],
      "metadata": {
        "id": "YDtRfVhwWhfz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanReview(text):\n",
        "  txt = text.translate(str.maketrans('','',string.punctuation))\n",
        "  txt = remove_non_english_words(txt)\n",
        "  return txt\n",
        "\n",
        "def readFiles(filename):\n",
        "    with open(filename,'r') as f:\n",
        "        texto = f.read()  \n",
        "    # Convert all text to lowercase\n",
        "    texto = texto.lower()\n",
        "    # Replace newline characters with empty string\n",
        "    texto = texto.replace('\\n', '')\n",
        "    # Find all tags in the text and remove closing tags\n",
        "    textohelp = re.sub(r'</.*?>', '', texto)\n",
        "    # Extract names of all tags in the text\n",
        "    etiquetas = re.findall(r'<.*?>', textohelp)\n",
        "    nombres_etiquetas = [re.sub(r'[<>]', '', etiqueta) for etiqueta in etiquetas]\n",
        "    # Find unique tags\n",
        "    etiquetas_unicas = set(nombres_etiquetas)\n",
        "    # Create a BeautifulSoup object from the text\n",
        "    soup = BeautifulSoup(texto, 'html.parser')\n",
        "    # Find all review elements and create a Python object\n",
        "    reviews =soup.find_all('review')\n",
        "    return  reviews\n",
        "\n",
        "def loadReviews(revs,reviewType):\n",
        "  reviewList = []\n",
        "  for a in revs:\n",
        "        objeto = {}\n",
        "        # Replace newline characters with empty string\n",
        "        textoaux = a.prettify().replace('\\n', '')\n",
        "        if len(textoaux)>3000 or len(textoaux)<10 :\n",
        "            continue\n",
        "        try:\n",
        "             # code that might raise an error if read a unknown character\n",
        "            raiz = ET.fromstring(textoaux)\n",
        "        except:\n",
        "            continue\n",
        "        # Mapping the new object\n",
        "        if raiz.find('rating') is not None:\n",
        "            objeto['rating'] = raiz.find('rating').text\n",
        "        if raiz.find('review_text') is not None:\n",
        "            objeto['review_text'] = cleanReview(raiz.find('review_text').text)\n",
        "        objeto['result']=reviewType\n",
        "        \n",
        "        reviewList.append(objeto)\n",
        "  return reviewList\n",
        "\n",
        "\n",
        "# result = 0 NEGATIVE\n",
        "raw_dataseta = readFiles('./negative.review')\n",
        "negRevs = loadReviews(raw_dataseta,0)\n",
        "\n",
        "# result = 1 POSITIVE\n",
        "raw_datasetb =  readFiles('./positive.review')\n",
        "posRevs = loadReviews(raw_datasetb,1)\n",
        "\n",
        "# Full dataset NEG & POS reviews\n",
        "data = negRevs + posRevs \n",
        "df = pd.DataFrame(data)\n",
        "dataset = df.dropna()\n",
        "dataset = dataset.copy()\n",
        "\n",
        "print(len(dataset))\n",
        "# Randomize the dataset\n",
        "dataset = dataset.reindex(np.random.permutation(dataset.index))\n",
        "dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "Vn0VEnmERVGf",
        "outputId": "eccba9a3-db53-4dff-c2ac-296b9ef42a47"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1954\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      rating                                        review_text  result\n",
              "487     1.0   these were are a p o you dont know what is it ...       0\n",
              "130     1.0   though i like the keyboard found two serious w...       0\n",
              "684     1.0   i bought this product less than a year ago day...       0\n",
              "378     2.0   it flashy and on the web shopping that feature...       0\n",
              "1099    5.0   great quality excellent construction and stron...       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3fe407b2-b9bb-4c24-897c-091d39a1b050\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>review_text</th>\n",
              "      <th>result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>487</th>\n",
              "      <td>1.0</td>\n",
              "      <td>these were are a p o you dont know what is it ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>1.0</td>\n",
              "      <td>though i like the keyboard found two serious w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>684</th>\n",
              "      <td>1.0</td>\n",
              "      <td>i bought this product less than a year ago day...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>2.0</td>\n",
              "      <td>it flashy and on the web shopping that feature...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1099</th>\n",
              "      <td>5.0</td>\n",
              "      <td>great quality excellent construction and stron...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3fe407b2-b9bb-4c24-897c-091d39a1b050')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3fe407b2-b9bb-4c24-897c-091d39a1b050 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3fe407b2-b9bb-4c24-897c-091d39a1b050');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove repeating words\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence),deacc=True))"
      ],
      "metadata": {
        "id": "1UduOyKYSoWA"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detokenize(text):\n",
        "    return TreebankWordDetokenizer().detokenize(text)"
      ],
      "metadata": {
        "id": "dcxX4tKuSwlq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = []\n",
        "#Splitting pd.Series to list\n",
        "data_to_list = dataset['review_text'].values.tolist()\n",
        "for i in range(len(data_to_list)):\n",
        "    temp.append((data_to_list[i]))\n",
        "data_words = list(sent_to_words(temp))\n",
        "print(data_words[:5])\n",
        "data = []\n",
        "for i in range(len(data_words)):\n",
        "    data.append(detokenize(data_words[i]))\n",
        "print(data[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8TUYVjVSxYo",
        "outputId": "3864d7a9-3078-4903-db60-cda30b4c2bc5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['these', 'were', 'are', 'you', 'dont', 'know', 'what', 'is', 'it', 'because', 'cant', 'say', 'it', 'on', 'basically', 'the', 'layer', 'an', 'offset', 'the', 'do', 'not', 'match', 'in', 'position', 'this', 'will', 'give', 'you', 'if', 'you', 'have', 'file', 'which', 'is', 'directly', 'on', 'layer', 'break', 'or', 'at', 'the', 'end', 'of', 'the', 'disk', 'do', 'not', 'buy', 'repeat', 'do', 'not', 'not', 'sure', 'why', 'their', 'crummy', 'product'], ['though', 'like', 'the', 'keyboard', 'found', 'two', 'serious', 'with', 'the', 'mouse', 'first', 'it', 'is', 'very', 'sensitive', 'to', 'movement', 'tried', 'making', 'some', 'but', 'it', 'is', 'still', 'difficult', 'to', 'control', 'finally', 'and', 'more', 'importantly', 'the', 'mouse', 'set', 'of', 'aa', 'about', 'every', 'days', 'only', 'use', 'the', 'mouse', 'about', 'week', 'so', 'should', 'be', 'getting', 'lot', 'more', 'battery', 'life', 'bought', 'other', 'and', 'have', 'been', 'with', 'them', 'but', 'in', 'this', 'case', 'wish', 'had', 'bought', 'another', 'brand'], ['bought', 'this', 'product', 'less', 'than', 'year', 'ago', 'days', 'ago', 'locked', 'my', 'and', 'tried', 'to', 'unlock', 'it', 'but', 'it', 'was', 'somehow', 'up', 'and', 'the', 'combination', 'did', 'not', 'work', 'had', 'to', 'cut', 'it', 'dont', 'know', 'how', 'it', 'got', 'up', 'but', 'sure', 'that', 'it', 'was', 'not', 'study', 'enough', 'to', 'last', 'long'], ['it', 'flashy', 'and', 'on', 'the', 'web', 'shopping', 'that', 'featured', 'it', 'however', 'after', 'setting', 'up', 'first', 'of', 'all', 'even', 'in', 'this', 'day', 'and', 'age', 'its', 'not', 'though', 'the', 'call', 'center', 'representative', 'had', 'assured', 'me', 'after', 'with', 'her', 'superior', 'that', 'it', 'indeed', 'was', 'so', 'cannot', 'comment', 'on', 'meanwhile', 'these', 'mobile', 'were', 'the', 'main', 'reason', 'this', 'particular', 'the', 'built', 'in', 'digital', 'message', 'recorder', 'is', 'of', 'the', 'quality', 'was', 'looking', 'for', 'an', 'alternative', 'to', 'the', 'monthly', 'charge', 'for', 'voice', 'mail', 'through', 'the', 'phone', 'company', 'this', 'it', 'if', 'you', 'prefer', 'to', 'record', 'your', 'own', 'message', 'in', 'lieu', 'of', 'the', 'generic', 'message', 'your', 'will', 'here', 'something', 'sounding', 'like', 'recording', 'it', 'struck', 'me', 'as', 'odd', 'that', 'with', 'the', 'included', 'audio', 'cable', 'you', 'can', 'connect', 'the', 'handset', 'to', 'an', 'audio', 'device', 'with', 'headphone', 'jack', 'to', 'record', 'custom', 'ring', 'with', 'good', 'quality', 'yet', 'to', 'record', 'message', 'for', 'the', 'digital', 'machine', 'you', 'need', 'to', 'speak', 'into', 'the', 'dinky', 'little', 'microphone', 'hole', 'on', 'the', 'base', 'unit', 'with', 'the', 'low', 'sound', 'quality', 'finally', 'the', 'battery', 'life', 'of', 'the', 'handset', 'is', 'rather', 'short', 'this', 'is', 'despite', 'the', 'too', 'low', 'on', 'the', 'handset', 'oh', 'why', 'no', 'on', 'the', 'base', 'unit'], ['great', 'quality', 'excellent', 'construction', 'and', 'strong', 'have', 'worked', 'with', 'decent', 'share', 'of', 'and', 'have', 'never', 'had', 'to', 'cut', 'and', 'terminate', 'cable', 'due', 'to', 'regular', 'wear', 'and', 'tear']]\n",
            "['these were are you dont know what is it because cant say it on basically the layer an offset the do not match in position this will give you if you have file which is directly on layer break or at the end of the disk do not buy repeat do not not sure why their crummy product', 'though like the keyboard found two serious with the mouse first it is very sensitive to movement tried making some but it is still difficult to control finally and more importantly the mouse set of aa about every days only use the mouse about week so should be getting lot more battery life bought other and have been with them but in this case wish had bought another brand', 'bought this product less than year ago days ago locked my and tried to unlock it but it was somehow up and the combination did not work had to cut it dont know how it got up but sure that it was not study enough to last long', 'it flashy and on the web shopping that featured it however after setting up first of all even in this day and age its not though the call center representative had assured me after with her superior that it indeed was so cannot comment on meanwhile these mobile were the main reason this particular the built in digital message recorder is of the quality was looking for an alternative to the monthly charge for voice mail through the phone company this it if you prefer to record your own message in lieu of the generic message your will here something sounding like recording it struck me as odd that with the included audio cable you can connect the handset to an audio device with headphone jack to record custom ring with good quality yet to record message for the digital machine you need to speak into the dinky little microphone hole on the base unit with the low sound quality finally the battery life of the handset is rather short this is despite the too low on the handset oh why no on the base unit', 'great quality excellent construction and strong have worked with decent share of and have never had to cut and terminate cable due to regular wear and tear']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to clean the data and deleate stop words\n",
        "def text_procesing(text):\n",
        "  STOPWORDS = set(stopwords.words('english'))\n",
        "  cleaned_reviews = []\n",
        "  for review in text:\n",
        "    tokens = [word for word in word_tokenize(review) if not word in STOPWORDS]\n",
        "    cleaned_reviews.append(\" \".join(tokens))\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lem_reviews = []\n",
        "  for review in cleaned_reviews:\n",
        "    lem_reviews.append(\" \".join(list(map(lemmatizer.lemmatize, word_tokenize(review)))))\n",
        "  return lem_reviews"
      ],
      "metadata": {
        "id": "k_gLvYKBYirz"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WnJ-ASUQZUaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = dataset['review_text']\n",
        "dataset['review_text'] =text_procesing(y)"
      ],
      "metadata": {
        "id": "8o4tdmskjIWz"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 5000\n",
        "max_len = 200\n",
        "# Create the token array and their sequences\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(dataset['review_text'])\n",
        "sequences = tokenizer.texts_to_sequences(dataset['review_text'])\n",
        "reviews_sequences = sequence.pad_sequences(sequences, maxlen=max_len)\n",
        "print(reviews_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nznNW0IqZ0W6",
        "outputId": "82333d86-d9fe-4036-fc7a-12b9190e0570"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   0    0    0 ...  116 3318   10]\n",
            " [   0    0    0 ...   12   43  121]\n",
            " [   0    0    0 ...  114  123  106]\n",
            " ...\n",
            " [   0    0    0 ... 2090  583  550]\n",
            " [   0    0    0 ...   95  119    7]\n",
            " [   0    0    0 ... 1290  693  436]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Y input variable sentiment categories \n",
        "result_sentiment = keras.utils.to_categorical(dataset['result'].astype('float32'), 2, dtype=\"float32\")\n",
        "print(result_sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58GZyNQLihQ_",
        "outputId": "5cc114b8-09ca-4106-fc06-2c776671fb63"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " ...\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews_sequences,result_sentiment, random_state=0)\n",
        "print (len(X_train),len(X_test),len(y_train),len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5W3rGPkji37",
        "outputId": "a60eceb4-a449-4dc7-80d4-0a12ddffcae0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1465 489 1465 489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation of the NN Sequential model\n",
        "model0 = Sequential()\n",
        "embedding_layer = Embedding(1000, 64)\n",
        "model0.add(layers.Embedding(max_words, 15))\n",
        "model0.add(layers.SimpleRNN(15,return_sequences=True))\n",
        "model0.add(layers.SimpleRNN(15))\n",
        "model0.add(layers.Dense(2,activation='softmax'))"
      ],
      "metadata": {
        "id": "TUR76K17aZFe"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "md = Sequential()\n",
        "embedding_layer = Embedding(1000, 64)\n",
        "md.add(layers.Embedding(max_words, 15))\n",
        "md.add(layers.SimpleRNN(15,return_sequences=True))\n",
        "md.add(layers.SimpleRNN(15))\n",
        "md.add(layers.Dense(2,activation='softmax'))"
      ],
      "metadata": {
        "id": "ouImLqJN9UrP"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "md.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# history = model0.fit(X_train, y_train, epochs=10,callbacks=[PrintDot()])\n",
        "history = md.fit(X_train, y_train, epochs=10,validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96Ag0bCc9ZbJ",
        "outputId": "769e4e70-4121-4fb6-d985-7f2d517d8b68"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "46/46 [==============================] - 8s 133ms/step - loss: 0.6995 - accuracy: 0.5358 - val_loss: 0.6807 - val_accuracy: 0.5440\n",
            "Epoch 2/10\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.5862 - accuracy: 0.7215 - val_loss: 0.7035 - val_accuracy: 0.5603\n",
            "Epoch 3/10\n",
            "46/46 [==============================] - 5s 108ms/step - loss: 0.3824 - accuracy: 0.8628 - val_loss: 0.7745 - val_accuracy: 0.5665\n",
            "Epoch 4/10\n",
            "46/46 [==============================] - 5s 112ms/step - loss: 0.2049 - accuracy: 0.9352 - val_loss: 0.8522 - val_accuracy: 0.5849\n",
            "Epoch 5/10\n",
            "46/46 [==============================] - 4s 89ms/step - loss: 0.1035 - accuracy: 0.9713 - val_loss: 0.9842 - val_accuracy: 0.5542\n",
            "Epoch 6/10\n",
            "46/46 [==============================] - 6s 131ms/step - loss: 0.0565 - accuracy: 0.9870 - val_loss: 1.1143 - val_accuracy: 0.5419\n",
            "Epoch 7/10\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.0327 - accuracy: 0.9904 - val_loss: 1.2088 - val_accuracy: 0.5501\n",
            "Epoch 8/10\n",
            "46/46 [==============================] - 4s 97ms/step - loss: 0.0173 - accuracy: 0.9966 - val_loss: 1.3210 - val_accuracy: 0.5542\n",
            "Epoch 9/10\n",
            "46/46 [==============================] - 6s 127ms/step - loss: 0.0160 - accuracy: 0.9959 - val_loss: 1.3867 - val_accuracy: 0.5501\n",
            "Epoch 10/10\n",
            "46/46 [==============================] - 4s 87ms/step - loss: 0.0090 - accuracy: 0.9966 - val_loss: 1.4627 - val_accuracy: 0.5746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model0.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# history = model0.fit(X_train, y_train, epochs=10,callbacks=[PrintDot()])\n",
        "history = model0.fit(X_train, y_train, epochs=10)\n",
        "# validation_data=(X_test, y_test))\n",
        "# history = model0.fit(X_train, y_train, epochs=10,validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pTGOC4xaczf",
        "outputId": "99cb489d-8948-48dc-e6bd-2516215b6ab9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "46/46 [==============================] - 7s 95ms/step - loss: 0.6872 - accuracy: 0.5604\n",
            "Epoch 2/10\n",
            "46/46 [==============================] - 4s 80ms/step - loss: 0.5093 - accuracy: 0.7788\n",
            "Epoch 3/10\n",
            "46/46 [==============================] - 4s 96ms/step - loss: 0.2917 - accuracy: 0.8962\n",
            "Epoch 4/10\n",
            "46/46 [==============================] - 5s 100ms/step - loss: 0.1657 - accuracy: 0.9522\n",
            "Epoch 5/10\n",
            "46/46 [==============================] - 4s 83ms/step - loss: 0.0842 - accuracy: 0.9768\n",
            "Epoch 6/10\n",
            "46/46 [==============================] - 4s 91ms/step - loss: 0.0461 - accuracy: 0.9870\n",
            "Epoch 7/10\n",
            "46/46 [==============================] - 5s 102ms/step - loss: 0.0278 - accuracy: 0.9925\n",
            "Epoch 8/10\n",
            "46/46 [==============================] - 4s 83ms/step - loss: 0.0136 - accuracy: 0.9973\n",
            "Epoch 9/10\n",
            "46/46 [==============================] - 4s 91ms/step - loss: 0.0098 - accuracy: 0.9986\n",
            "Epoch 10/10\n",
            "46/46 [==============================] - 5s 107ms/step - loss: 0.0076 - accuracy: 0.9986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREDICTIONS"
      ],
      "metadata": {
        "id": "0SFq69wK4hJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment = ['Negative','Positive']"
      ],
      "metadata": {
        "id": "wkgGORdtkdN5"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_input = input(\"WRITE THE REVIEW:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpc3nweEBI6H",
        "outputId": "4f43199c-8ee1-4475-cf86-5c105e952928"
      },
      "execution_count": 74,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WRITE THE REVIEW:I never even traveled with it. It just stopped working completely making me lose data.  NEVER AGAIN I'll buy anything from this company. This drive is a piece of rubbish!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NEGATIVE Test reviewa\n",
        "review_input= 'the product is horrible, and useless'\n",
        "# review_input= 'this experience has been the worst , want all my money back'\n",
        "# review_input='I never even traveled with it. It just stopped working completely making me lose data. NEVER AGAIN I'll buy anything from this company. This drive is a piece of rubbish!'\n",
        "\n",
        "# POSITIVE Test reviewa\n",
        "# review_input = 'The product makes good smothies and also is great for juices'\n",
        "# review_input= 'definitely recommend this memory card, download time is great'\n",
        "# review_input= 'I borrowed this book on CD from the library and half way through, ordered the book so I would have it to refer to. Excellent book, highly recommended. Helps to put a realistic perspective on millionaires'\n",
        "\n"
      ],
      "metadata": {
        "id": "vxp454pElsya"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequencer= tokenizer.texts_to_sequences([review_input])"
      ],
      "metadata": {
        "id": "KSnDtDC3BVBX"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL WITHOUT VALIDATION DATA\n",
        "test= sequence.pad_sequences(sequencer, maxlen=max_len)\n",
        "predictions =model0.predict(test) \n",
        "print(predictions)\n",
        "sentiment[np.around(predictions, decimals=0).argmax(axis=1)[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "jUwckRq-l1T9",
        "outputId": "4a623373-4dd3-4465-9839-fa03542b802b"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 63ms/step\n",
            "[[9.992968e-01 7.031906e-04]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Negative'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL WITH VALIDATION DATA\n",
        "test= sequence.pad_sequences(sequencer, maxlen=max_len)\n",
        "predictions =md.predict(test) \n",
        "print(predictions)\n",
        "sentiment[np.around(predictions, decimals=0).argmax(axis=1)[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "5AiWSQOu9-Cj",
        "outputId": "c2cd95c5-b415-41fa-cad5-a631159cd342"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 35ms/step\n",
            "[[0.99721354 0.0027865 ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Negative'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    }
  ]
}